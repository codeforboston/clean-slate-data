---
output: pdf_document
---
This script repeats the analyses used to initially select PA as a proxy for MA crime rates, substituting the 2014 data used in the original script with new data from 2018. To do so, we calculate Euclidian distance between two-state (MA & all 50 others (inc. DC)) matrices, using both standardized and raw rates of crime.

So first things first, let's load our libraries and data:
```{r}
knitr::opts_chunk$set(warning = F)
library(here) ## relative pathways
library(readxl) ## reading excel
library(dplyr) ## data manipulation
library(tidyr) ## nesting dataframes
library(purrr) ## map reduce functions
FBIData <- read_excel("../../data/cleaned/fbi_aggregated_2018/2018_FBI_aggregate_crime_data.xlsx")
```

The first thing that was done with the 2014 data was to calculate the euclidian distance between each state and Massachusetts. Now, we're using a different data source altogether here, so I have to take a bit of a different approach in my wrangling -- most importantly, I have to calculate the per-capita arrest rate myself (I'm assuming these numbers aren't per capita or 100k based on California reporting about 1 million arrests for its 40 million population).
```{r}
nestedFBIData <- FBIData %>%
  filter(age_category == "Under 18") %>%
  ## these variables are the same ones done with 2014 data
  select(state, 
         year, 
         robbery, 
         property_crime, 
         burglary, 
         larceny_theft, 
         motor_vehicle_theft, 
         estimated_population) %>%
  ## get per-capita crime rate
  mutate_if(is.numeric, funs(. / estimated_population)) %>%
  select(-estimated_population) %>%
  ## this bit is wonky if you don't know R; I'm creating a column of dataframes
  ## containing data for only that state
  nest(data = -c(state)) %>%
  rename(other_states = data)

## this is decidedly not the smartest way to do this, but:
## iterate across each state, combining its dataframe with the MA one;
## create a list of those dataframes in "frames"
frames <- vector("list")
for (i in seq_along(nestedFBIData$state)) {
  frames[i] <- nest(data = everything(), 
                    rbind(nestedFBIData[[i, "other_states"]], 
                          nestedFBIData[nestedFBIData$state == "Massachusetts", ][[2]][[1]]))
}

## now iterate through those dataframes calulating euclidian distance
## (same metric as last time)
distScore <- vector()
for (i in seq_along(frames)) {
  distScore[i] <- dist(frames[[i]][[1]])
}
## now let's label those scores and examine the results!
distScores <- as_tibble(cbind(nestedFBIData$state, distScore)) %>%
  arrange(distScore) %>%
  rename(State = V1)
distScores
```

So PA in this quick experiment comes in 10th place (if you count MA as 1st; which is... a choice) -- which is still 80th percentile, but I'm wondering if we had pragmatic considerations for selecting PA in addition to distance-based ones. I think that, unless it turns out that New Jersey publishes all their crimes in a Google Spreadsheet emailed to everyone on New Year's, we can still justify looking at PA with this result -- the distinction between any top 10 state other than NJ and maybe Kentucky is extremely minimal.

Of course, the last analysis standardized the variables we looked at as well, so that rates of offense among less common crimes could be weighted equally to more common offenses. I need to do an inch more reading to have an opinion on this, I think -- it seems to me like we should care about the crimes with more absolute cases more here (and weight them accordingly), as our desired outcome isn't as much "what state has the same offender profiles" as it is "what has the same rate of crimes" -- that is, I think that a large percentage difference in a more common crime category is more important to us than a similar magnitude but smaller absolute number difference in a less common one. But I'm not entirely sure, so here's the analysis run with standardized data:
```{r}
nestedFBIData <- FBIData %>%
  filter(age_category == "Under 18" &
    state != "Iowa") %>%
  select(state, 
         year, 
         robbery, 
         property_crime, 
         burglary, 
         larceny_theft, 
         motor_vehicle_theft, 
         estimated_population) %>%
  mutate_if(is.numeric, funs(. / estimated_population)) %>%
  mutate_if(is.numeric, scale) %>%
  select(-estimated_population) %>%
  nest(data = -c(state)) %>%
  rename(other_states = data)

frames <- vector("list")
for (i in seq_along(nestedFBIData$state)) {
  frames[i] <- nest(data = everything(), 
                    rbind(nestedFBIData[[i, "other_states"]], 
                          nestedFBIData[nestedFBIData$state == "Massachusetts", ][[2]][[1]]))
}

## now iterate through those dataframes calulating euclidian distance
## (same metric as last time)
distScoreStand <- vector()
for (i in seq_along(frames)) {
  distScoreStand[i] <- dist(frames[[i]][[1]])
}
## now let's label those scores and examine the results!
standScores <- as_tibble(cbind(nestedFBIData$state, distScoreStand)) %>%
  arrange(distScoreStand) %>%
  rename(State = V1)

standScores

standScores %>%
  tail(-10)
```

PA now is number 12, down two spots. Notably, Vermont came from almost dead last to an undisputable second place here. Having Vermont and New Hampshire here makes an amount of sense to me -- those are extremely similar states, after all. I'll look forward to talking about this with people on Tuesday -- I'm not even clear myself on what this implies for our next steps.

```{r}
library(ggplot2)
library(ggrepel)
library(forcats)
graphFrame <- distScores %>%
  full_join(standScores) %>%
  drop_na()

graphFrame %>%
  mutate(abb = setNames(state.name, state.name)[State]) %>%
  mutate(distScore = as.numeric(distScore),
         distScoreStand = as.numeric(distScoreStand),
         abb = factor(abb, levels = c("Pennsylvania", "Kentucky", "New Jersey", "Virginia", "Ohio", "Michigan", state.name[!(state.name %in% c("Kentucky", "New Jersey", "Virginia", "Pennsylvania", "Ohio", "Michigan"))]))) %>%
  ggplot(aes(distScore, distScoreStand)) + 
  geom_text_repel(aes(label = abb, color = abb), segment.colour = NA, show.legend = F) +
  scale_color_manual(values = c(rep("black", 1), rep("grey90", 48))) +
  scale_y_continuous(limits = c(0, 6.5),
                     breaks = c(0, 6),
                     labels = c("Most\nSimilar", "Least\nSimilar")) + 
  scale_x_continuous(limits = c(0, 0.003),
                     breaks = c(0, 0.0028),
                     labels = c("Most Similar", "Least Similar")) +
  theme_classic() %+replace%
  theme() +
  labs(x = "Similarity -- Most Common Offenses",
       y = "Similarity -- All Offenses")
```
