{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook handles the processing of PA docket data that has been downloaded in JSON format and converted into a CSV with the following columns:\n",
    "\n",
    "* docket_no: Court docket number\n",
    "* status: Status of this docket\n",
    "* gender: Offender's gender\n",
    "* race: Offender's race\n",
    "* county: County of the court managing this docket\n",
    "* offender_id: Hashed value for the Offender\n",
    "* offense_age: Age computed from DOB\n",
    "* seq_no: Sequential numbering of charges\n",
    "* statute: Statute code in violation\n",
    "* grade: Grade of the crime\n",
    "* statute_description: Statute description\n",
    "* offense_date: Date of the offense\n",
    "* description: Most likely the same as statute description\n",
    "* offense_tracking_no: Tracking number for the offense for multiple offenders involved\n",
    "* disposition: Disposition of the charge\n",
    "* sentence_date: Sentencing date (if any)\n",
    "* sentence_start: Start of the sentence to be served (if any)\n",
    "* sentence_type: Type of the sentence meted (if any)\n",
    "* sentence_min_pd: Minimum sentence (if any)\n",
    "* sentence_max_pd: Maximum sentence (if any)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bio(json_data):\n",
    "    \"\"\"\n",
    "    Retrieves the biographical information\n",
    "    \"\"\"\n",
    "    return dict(\n",
    "        docket_no = json_data[\"docketNumber\"],\n",
    "        status = json_data[\"statusName\"],\n",
    "        gender = json_data[\"caseParticipants\"][0][\"gender\"],\n",
    "        dob = json_data[\"caseParticipants\"][0][\"primaryDateOfBirth\"],\n",
    "        race = json_data[\"caseParticipants\"][0][\"race\"],\n",
    "        first_name = json_data[\"caseParticipants\"][0][\"participantName\"][\"firstName\"],\n",
    "        middle_name = json_data[\"caseParticipants\"][0][\"participantName\"][\"middleName\"],\n",
    "        last_name = json_data[\"caseParticipants\"][0][\"participantName\"][\"lastName\"],\n",
    "        county = json_data[\"county\"][\"name\"]\n",
    "    )\n",
    "    \n",
    "def get_offenses(json_data):\n",
    "    \"\"\"\n",
    "    Retrieves the list of offenses\n",
    "    \"\"\"\n",
    "    offenses = map(\n",
    "        lambda x: (\n",
    "            x[\"sequenceNumber\"], \n",
    "            x[\"statuteName\"], \n",
    "            x[\"grade\"], \n",
    "            x[\"statuteDescription\"], \n",
    "            x[\"offenseDate\"], \n",
    "            x[\"description\"], \n",
    "            x[\"otn\"]), \n",
    "        json_data[\"offenses\"])\n",
    "    return pd.DataFrame(\n",
    "        offenses, \n",
    "        columns=['seq_no', 'statute', 'grade', 'statute_description', 'offense_date', \n",
    "                 'description', 'offense_tracking_no'])\n",
    "\n",
    "def get_dispositions(json_data):\n",
    "    \"\"\"Retrieves the disposition (if applicable) of the offenses\"\"\"\n",
    "    def process_sentencing(sentence_section):\n",
    "        \"\"\"Extracts sentencing as part of the disposition\"\"\"\n",
    "        if len(sentence_section) == 0:\n",
    "            return (None, None, None, None, None)\n",
    "        else:\n",
    "            latest_sentence = sentence_section[-1]\n",
    "            return (latest_sentence[\"eventDate\"], \n",
    "                latest_sentence[\"sentenceTypes\"][0][\"startDateTime\"],\n",
    "                latest_sentence[\"sentenceTypes\"][0][\"sentenceType\"],\n",
    "                latest_sentence[\"sentenceTypes\"][0][\"minPeriod\"],\n",
    "                latest_sentence[\"sentenceTypes\"][0][\"maxPeriod\"])\n",
    "    \n",
    "    if len(json_data[\"dispositionEvents\"]) > 0:\n",
    "        disposition_section = json_data[\"dispositionEvents\"][-1][\"offenseDispositions\"]\n",
    "        dispositions = map(lambda x: (\n",
    "            x[\"sequenceNumber\"], \n",
    "            x[\"disposition\"]) +\n",
    "            process_sentencing(x[\"sentences\"]), disposition_section)    \n",
    "    else:\n",
    "        dispositions = None\n",
    "    return pd.DataFrame(\n",
    "        dispositions,\n",
    "        columns=['seq_no', 'disposition', 'sentence_date', 'sentence_start', \n",
    "                 'sentence_type', 'sentence_min_pd', 'sentence_max_pd']\n",
    "    )\n",
    "\n",
    "def offense_age(row):\n",
    "    \"\"\"Computes the age of the offender at the time of the offense\"\"\"\n",
    "    \n",
    "    if row[\"offense_date\"] is pd.NaT or row[\"dob\"] is pd.NaT:\n",
    "        # If the date is not valid return None\n",
    "        return None\n",
    "    else: \n",
    "        # Else get the number of years between offense date and DOB\n",
    "        return relativedelta(row[\"offense_date\"].date(), row[\"dob\"].date()).years\n",
    "    \n",
    "def get_records(json_data):\n",
    "    \"\"\"Pieces together all relevant pieces from the docket\"\"\"\n",
    "    \n",
    "    # Retrieve components of the data\n",
    "    bio = get_bio(json_data) # Biographical information\n",
    "    off = get_offenses(json_data) # Charges\n",
    "    disps = get_dispositions(json_data) # Disposition of the charges\n",
    "    \n",
    "    # Merge the data together\n",
    "    merged = off.merge(disps, on=\"seq_no\", how='left')\n",
    "    \n",
    "    # Federate out the biographical data so this is de-normalized\n",
    "    for k, v in get_bio(json_data).items():\n",
    "        merged[k] = v\n",
    "    \n",
    "    # Convert date fields into datetime\n",
    "    merged[\"dob\"] = pd.to_datetime(merged[\"dob\"], errors = 'coerce')\n",
    "    merged[\"offense_date\"] = pd.to_datetime(merged[\"offense_date\"], errors = 'coerce')\n",
    "    merged[\"sentence_date\"] = pd.to_datetime(merged[\"sentence_date\"], errors = 'coerce')\n",
    "    \n",
    "    # Construct a unique ID by hashing the names and DOB\n",
    "    uid_str = \"\".join(filter(None, (bio[\"first_name\"], bio[\"middle_name\"], bio[\"last_name\"], bio[\"dob\"])))\n",
    "    merged[\"offender_id\"] = hashlib.sha256(uid_str.encode(\"utf-8\")).hexdigest()[:12]\n",
    "    \n",
    "    # Compute age at time of each offense\n",
    "    merged[\"offense_age\"] = merged.apply(offense_age, axis=1)\n",
    "    \n",
    "    # Drop sensitive columns\n",
    "    merged = merged.drop(columns=[\"first_name\", \"middle_name\", \"last_name\", \"dob\"])\n",
    "    \n",
    "    # Re-order columns\n",
    "    cols = merged.columns.tolist()\n",
    "    cols = cols[len(cols)-7:] + cols[0:-7]\n",
    "    return merged[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "163277ea75a64f85b7813c08650e11d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=68481), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_path = \"data/pa_json/\"\n",
    "output_path = \"data/output/\"\n",
    "appended_data = []\n",
    "\n",
    "def process_file(json_file):\n",
    "    with open(json_file) as f:\n",
    "        try:\n",
    "            data = json.load(f)\n",
    "            appended_data.append(get_records(data))\n",
    "        except:\n",
    "            print(json_file)\n",
    "            raise\n",
    "            \n",
    "for i, input_file in enumerate(tqdm_notebook(os.listdir(input_path))):\n",
    "    if input_file.endswith(\".json\"):\n",
    "        process_file(path + input_file)\n",
    "    if i > 0 and i % 10000 == 0:\n",
    "        df = pd.concat(appended_data)\n",
    "        df.to_csv(f\"data/output/pa_data_{i}.csv\")\n",
    "        appended_data = []\n",
    "\n",
    "df = pd.concat(appended_data)\n",
    "df.to_csv(f\"{output_path}pa_data_{i}.csv\")\n",
    "appended_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pa_data = pd.concat([pd.read_csv(f\"{output_path}{x}\", low_memory=False) for x in os.listdir(output_path)], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mapping for salted/hashed docket id\n",
    "import os\n",
    "salt = os.urandom(32)\n",
    "hashed_docket_id = pa_data.apply(lambda row: hashlib.sha256(f\"{salt}{row['docket_no']}\".encode(\"utf-8\")).hexdigest()[:12], axis=1)\n",
    "docket_map = pd.concat([hashed_docket_id, pa_data[\"docket_no\"]], axis=1)\n",
    "docket_map.columns = [\"hash_docket_no\",\"real_docket_no\"]\n",
    "docket_map.drop_duplicates().to_csv(f\"{output_path}docket_mapping.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace docket number\n",
    "pa_data[\"docket_no\"] = hashed_docket_id\n",
    "pa_data = pa_data.drop([\"Unnamed: 0\"], axis=1)\n",
    "pa_data.to_csv(f\"{output_path}pa_data_all.csv.gz\", compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11537519"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pa_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
