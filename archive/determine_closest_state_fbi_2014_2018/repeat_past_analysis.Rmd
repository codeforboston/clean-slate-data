---
output: pdf_document
---
We're looking to expand our proxy-state analysis to additional years, in order to make sure we're comfortable using PA as a proxy for MA crime. Here we're combining data from 2014-2018 (from what was usually known as table 69, aggregated arrest rates, save one year it was reported as table 2) and repeating our look at euclidian distance vs MA with both standardized and non-standardized variables.

So first things first, let's load our libraries and data:
```{r}
knitr::opts_chunk$set(warning = F)
library(here) ## relative pathways
library(readxl) ## reading excel
library(dplyr) ## data manipulation
library(tidyr) ## nesting dataframes
library(purrr) ## map reduce functions
library(ggplot2)
library(magrittr)
fbi_data <- read_excel(
  here(
    "data", 
    "cleaned", 
    "fbi_aggregated_data_combined/FBI_aggregate_crime_data_2014_2018.xlsx"
    )
  )
```

Now we're repeating the same analysis done in the other FBI folder here, 
but nesting across state and year (so we'll compare each each state in 
2018 to MA 2018). 

```{r}
nested_fbi_data <- fbi_data %>%
  filter(age_category == "Under 18") %>%
  ## these variables are the same ones done with 2014 data
  select(state, 
         year, 
         robbery, 
         property_crime, 
         burglary, 
         larceny_theft, 
         motor_vehicle_theft, 
         estimated_population) %>%
  mutate(year = as.character(year)) %>%
  ## get per-capita crime rate
  mutate_if(is.numeric, funs(. / estimated_population)) %>%
  select(-estimated_population) %>%
  ## this bit is wonky if you don't know R; I'm creating a column of dataframes
  ## containing data for only that state
  nest(nested = -c(state, year)) %>%
  mutate(state = regmatches(state, 
                            regexpr("[[:alpha:]]*\\s?[[:alpha:]]*", 
                                    state)))
```

Calculate each state's distance from MA:
```{r}
ranked_distances <- nested_fbi_data %>%
  left_join(nested_fbi_data %>%
  filter(state == "Massachusetts") %>%
    select(-state, mass = nested),
  by = "year") %>%
  mutate(dist_tables = map2(nested, mass, vctrs::vec_rbind),
         dist_score = map_dbl(dist_tables, dist),
          year = as.numeric(year)) %>%
  filter(!is.infinite(dist_score) & 
           state != "Massachusetts") %>%
  arrange(year, dist_score) %>%
  group_by(year) %>%
  # y is the rank of the state for that year -- a rank of 1 means its
  # the most similar to MA that year
  mutate(y = seq(1, 49)) %>%
    ungroup() 

ranked_distances
```

And quickly graph that:
```{r}
ranked_distances %>%
  ggplot(aes(year, y, color = state)) +
  geom_line()
```

Gross! Let's quickly graph that better:
```{r}
ranked_distances %>%
  filter(y < 11) %>%
  ggplot(aes(year, y, color = state)) +
  geom_line()
```

"Better", at any rate. I'm going to set aside visualizations for a second and 
just pull PA rankings:

```{r}
ranked_distances %>%
  filter(state == "Pennsylvania")
```

So PA almost never cracks the top 10, except for 2016. However, it only drops 
out of the top 15 once -- and that's 2017, when a few other states have wild 
variances in our first graph. I wonder what the average ranking is across the 
board -- looks like most states are pretty stable:

```{r}
library(magrittr)
ranked_distances %>%
  group_by(state) %>%
  summarise(mean_rank = mean(y), median_rank = median(y)) %>%
  arrange(mean_rank) %T>%
  write.csv("state_ranks.csv")
```

I'm frankly shocked that WV is in the top bracket; KY is also surprising, but 
the rest of the states make sense. It also feels to me like we've got a few 
tiers here -- the top-tier proxies are states from WV to KY, which are typically 
in that top five bucket. Then the secondary tier (RI -> HI, maybe) hovers around
13. 

PA is in that second tier, with a mean ranking of 13.2 (again, 1 is closest) and
median ranking of... 13. It feels valuable to me to spend time looking into how 
accessible data for those top-tier states might be -- but if not, I think we've 
reinforced that PA is a decent option nontheless.

For completeness, I should make the same table based on standardized crime 
rates:

```{r}
nested_scaled_data <- fbi_data %>%
  filter(age_category == "Under 18") %>%
  ## these variables are the same ones done with 2014 data
  select(state, 
         year, 
         robbery, 
         property_crime, 
         burglary, 
         larceny_theft, 
         motor_vehicle_theft, 
         estimated_population) %>%
  mutate(year = as.character(year)) %>%
  ## get per-capita crime rate
  mutate_if(is.numeric, funs(. / estimated_population)) %>%
  mutate_if(is.numeric, scale) %>%
  select(-estimated_population) %>%
  ## this bit is wonky if you don't know R; I'm creating a column of dataframes
  ## containing data for only that state
  nest(nested = -c(state, year)) %>%
  mutate(state = regmatches(state, 
                            regexpr("[[:alpha:]]*\\s?[[:alpha:]]*", 
                                    state)))


ranked_scaled_distances <- nested_scaled_data %>%
  left_join(nested_scaled_data %>%
  filter(state == "Massachusetts") %>%
    select(-state, mass = nested),
  by = "year") %>%
  mutate(dist_tables = map2(nested, mass, rbind),
         dist_tables = map(dist_tables, as_tibble),
         dist_score = map_dbl(dist_tables, dist),
          year = as.numeric(year)) %>%
  filter(!is.infinite(dist_score) & 
           state != "Massachusetts") %>%
  arrange(year, dist_score) %>%
  group_by(year) %>%
  # y is the rank of the state for that year -- a rank of 1 means its
  # the most similar to MA that year
  mutate(y = seq(1, 50)) %>%
    ungroup()

ranked_scaled_distances
```

```{r}
ranked_scaled_distances %>%
  group_by(state) %>%
  summarise(mean_rank = mean(y), median_rank = median(y)) %>%
  arrange(mean_rank) %T>%
  write.csv("state_scaled_ranks.csv")
```

PA drops to 14th, and some weird contenders (ND? MT?) enter the upper tiers. I 
think I'm justified in saying unscaled data is probably a better approach, but I
don't think it changes much.